{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1a)\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C^n}{\\partial w_i} =  - \\frac{\\partial}{\\partial w_i}(y^n \\ln(\\hat y^n) + (1 - y^n)\\ln(1-\\hat y^n)) = - y^n \\frac{\\partial}{\\partial w_i} (\\ln(\\hat y^n)) - (1 - y^n) \\frac{\\partial}{\\partial w_i}(\\ln(1-\\hat y^n)))\n",
    "$$\n",
    "\n",
    "$$\n",
    "= - (\\frac{y^n}{\\hat y^n} - \\frac{1-y^n}{1-\\hat y^n})\\frac{\\partial \\hat y^n}{\\partial w_i} = - \\left(\\frac{y^n - \\hat y^n}{\\hat y^n (1- \\hat y^n)}\\right) \\frac{\\partial \\hat y^n}{\\partial w_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z} \\frac{1}{1 + e^z} = -(1 + e^z)^{-2} e^z\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat y^n}{\\partial w_i} = \\frac{\\partial}{\\partial w_i} \\frac{1}{1 + e^{-w^T x}} = -(1 + e^{-w^T x})^{-2} e^{-w^T x} \\frac{\\partial}{\\partial w_i} (-w^T x) = - (\\hat y^n)^2 (\\frac{1}{\\hat y^n} - 1) (-x_i) = \\hat y^n (1 - \\hat y^n)x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C^n}{\\partial w_i} = - \\left(\\frac{y^n - \\hat y^n}{\\hat y^n (1- \\hat y^n)}\\right) \\frac{\\partial \\hat y^n}{\\partial w_i} = - \\left(\\frac{y^n - \\hat y^n}{\\hat y^n (1- \\hat y^n)}\\right) \\hat y^n (1 - \\hat y^n)x_i = - (y^n - \\hat y^n) x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1b)\n",
    "$$\n",
    "\\frac{\\partial C^n}{\\partial w_{kj}} =  -\\frac{\\partial}{\\partial w_{kj}} \\sum_{i=1}^K y^n_i \\ln(\\hat y^n_i) = - \\sum_{i=1}^K \\frac{y^n_i}{\\hat y^n_i} \\frac{\\partial \\hat y^n_i}{\\partial w_{kj}} = - \\sum_{i=1}^K \\frac{y^n_i}{\\hat y^n_i} \\frac{\\partial \\hat y^n_i}{\\partial z_k} \\frac{\\partial z_k}{\\partial w_{kj}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat y^n_i}{\\partial z_k} (\\text{ for } i \\neq k) = \\frac{\\partial}{\\partial z_k} \\frac{e^{z_i}}{ e^{z_{k}} + \\sum_{k'\\neq k} e^{z_{k'}}} = e^{z_i} (-1) \\left(\\sum_{k'=1} e^{z_{k'}} \\right)^{-2} e^{z_k} = -\\hat y^n_i \\hat y^n_k\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat y^n_k}{\\partial z_k} = \\frac{\\partial}{\\partial z_k} \\frac{e^{z_k}}{\\sum_{k'=1}^K e^{z_{k'}}} =  \\frac{1}{\\sum_{k'=1}^K e^{z_{k'}}} \\frac{\\partial}{\\partial z_k} (e^{z_k})  +  e^{z_k} \\frac{\\partial}{\\partial z_k}\\left(\\sum_{k'=1}^K e^{z_{k'}}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "=  \\frac{e^{z_k}}{\\sum_{k'=1}^K e^{z_{k'}}}  +  e^{z_k} (-1) \\left(\\sum_{k'=1}^K e^{z_{k'}}\\right)^{-2} e^{z_k} = \\frac{e^{z_k}}{\\sum_{k'=1}^K e^{z_{k'}}} \\left(1 - \\frac{e^{z_k}}{\\sum_{k'=1}^K e^{z_{k'}}} \\right) = \\hat y^n_k(1-\\hat y^n_k) = \\hat y^n_k - \\hat y^n_k \\hat y^n_k\n",
    "$$\n",
    "\n",
    "$$\n",
    " \\frac{\\partial z_k}{\\partial w_{kj}} = x_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C^n}{\\partial w_{kj}} = - \\sum_{i=1}^K \\frac{y^n_i}{\\hat y^n_i} \\frac{\\partial \\hat y^n_i}{\\partial z_k} \\frac{\\partial z_k}{\\partial w_{kj}} = -\\frac{y^n_k}{\\hat y^n_k} \\hat  y^n_k  x_j - \\sum_{i=1}^K \\frac{y^n_i}{\\hat y^n_i} (-\\hat y^n_i \\hat y^n_k) x_j = x_j \\left( - y^n_k + \\hat y^n_k \\sum_{i=1}^K y^n_i \\right) = -x_j (y^n_k - \\hat y^n_k)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first tasks were done by using data that was sampled stochastically using the given code on github. This caused less pikes in the validation accuracy. However the stochastic sampling was turned off in task 2e to get some spikes to compare shuffling vs not shuffling.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "It stops after around 550 steps which is in the 19th epoch.\n",
    "\n",
    "\n",
    "![](task2d_binary_train_loss.png)\n",
    "![](task2d_binary_train_accuracy.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "When shuffling the training set one calculates the gradient on different parts of the data set each epoch making it a better representation of a real situation which again makes it better at the validation set.  \n",
    "![](task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![Softmax training loss variance](task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![Softmax training accuracy](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "From the plot of training accuracy compared to calidation accuracy, we can see some signs of overfitting. After around 3800 training steps, the validation accuracy stops improving and rather begins oscillating around the value $0.924$. However, the training accuracy is still increasing. This is undesired because we want our predictor to perform well on new data, rather than just the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(w)}{\\partial w_{kj}} = \\frac{\\partial}{\\partial w_{kj}} (C(w) + \\lambda R(w)) = \\frac{\\partial C}{\\partial w_{kj}} + \\lambda \\frac{\\partial R}{\\partial w_{kj}} \n",
    "$$\n",
    "$$=-x_j (y^n_k - \\hat y^n_k) + \\frac{\\lambda}{2} \\cdot \\Sigma_{i,j} \\frac{\\partial}{\\partial w_{kj}} w_{i,j}^2 = -x_j (y^n_k - \\hat y^n_k) + \\lambda \\cdot  w_{kj}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "When using regularization ($\\lambda=2$) the algorithm tries to keep the weights small while making good guesses. It will then prioritize to use the weights to extract the most important features of the image. E.g the weights for recognizing a 1 with regularization just recognizes  straight line i the middle, while the weights without regularization seems to try to recognize line with multiple gradients.  \n",
    "![j](task4b_softmax_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "\n",
    "With regularization the error have to be sufficiently large so that the regularization part of the gradient does not dominate so that the weights can be changed in the direction of better training accuracy. At some point the weights will become sufficiently large so that the gradient becomes very small while there still is errors to correct. This effect stops it from achieving better training accuracy which indirectly affect the validation accuracy.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "The L2-norm of the weights drastically decrease with increasing $\\lambda$.  \n",
    "![](task4d_l2_reg_norms.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
